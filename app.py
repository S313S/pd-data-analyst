import json
import os
import re
import subprocess
from dataclasses import dataclass, field
from typing import Any, Optional
from urllib.parse import parse_qs, urlparse

import requests
import streamlit as st
from bs4 import BeautifulSoup


USER_AGENT = (
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_2 like Mac OS X) "
    "AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1"
)
DESKTOP_USER_AGENT = (
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6_1) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/122.0.0.0 Safari/537.36"
)

URL_PATTERN = re.compile(r"https?://[^\s\"'<>\\]+", flags=re.I)
IMAGE_EXT_PATTERN = re.compile(r"\.(?:jpg|jpeg|png|webp|avif|gif)(?:$|\?)", flags=re.I)
VIDEO_EXT_PATTERN = re.compile(r"\.(?:mp4|m3u8|mov|webm)(?:$|\?)", flags=re.I)
VIDEO_HINTS = ("video", "play", "stream", "hls", "goods_video", "video_url")
IMAGE_HINTS = ("image", "img", "cover", "thumb", "pic")
STATIC_ASSET_EXT_PATTERN = re.compile(r"\.(?:js|css|map|json|html|htm|txt|xml)(?:$|\?)", flags=re.I)
BLOCKED_URL_KEYWORDS = (
    "down_download",
    "android_browser_download",
    "ios_fast_download",
    "need_popover=true",
    "itunes.apple.com",
    "apps.apple.com",
)
LOGIN_URL_KEYWORDS = ("login", "passport", "oauth", "verify", "sms")
STORAGE_STATE_FILE = os.path.join(os.getcwd(), ".playwright_storage_state.json")
PLAYWRIGHT_USER_DATA_DIR = os.path.join(os.getcwd(), ".playwright_user_data")


def is_blocked_jump_url(url: str) -> bool:
    low = url.lower()
    return any(k in low for k in BLOCKED_URL_KEYWORDS)


def apply_anti_detection_scripts(context: Any) -> None:
    try:
        context.add_init_script(
            """
            Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
            Object.defineProperty(navigator, 'languages', { get: () => ['zh-CN', 'zh', 'en-US', 'en'] });
            Object.defineProperty(navigator, 'platform', { get: () => 'MacIntel' });
            window.chrome = window.chrome || { runtime: {} };
            """
        )
    except Exception:
        pass


def cleanup_stale_test_browsers() -> dict[str, Any]:
    # Kill Chromium processes that are likely launched by Playwright for this project.
    patterns = [
        PLAYWRIGHT_USER_DATA_DIR,
        "--remote-debugging-pipe",
        "--disable-blink-features=AutomationControlled",
    ]
    matched_patterns: list[str] = []
    errors: list[str] = []
    for pattern in patterns:
        try:
            # rc=0 means at least one process matched and got signal.
            # rc=1 means no match; it's not an error for cleanup flows.
            result = subprocess.run(["pkill", "-f", pattern], check=False, capture_output=True, text=True)
            if result.returncode == 0:
                matched_patterns.append(pattern)
            elif result.returncode not in (0, 1):
                err = (result.stderr or result.stdout or "").strip()
                errors.append(f"{pattern}: rc={result.returncode} {err}")
        except Exception as exc:
            errors.append(f"{pattern}: {exc}")
    return {"matched_patterns": matched_patterns, "errors": errors}


def force_kill_chromium_processes() -> dict[str, Any]:
    try:
        result = subprocess.run(["pkill", "-x", "Chromium"], check=False, capture_output=True, text=True)
        return {
            "killed": result.returncode == 0,
            "returncode": result.returncode,
            "stderr": (result.stderr or "").strip(),
        }
    except Exception as exc:
        return {"killed": False, "returncode": -1, "stderr": str(exc)}


@dataclass
class ProductInfo:
    source_url: str
    final_url: str = ""
    title: str = ""
    images: list[str] = field(default_factory=list)
    videos: list[str] = field(default_factory=list)
    raw: dict[str, Any] = field(default_factory=dict)


def extract_url(text: str) -> str:
    pattern = re.compile(r"(https?://[^\s]+)")
    match = pattern.search(text.strip())
    if match:
        return match.group(1).strip("ï¼Œã€‚,.")
    return text.strip()


def normalize_url(raw: str) -> str:
    cleaned = extract_url(raw)
    if not cleaned:
        return ""
    parsed = urlparse(cleaned)
    if not parsed.scheme:
        cleaned = f"https://{cleaned}"
    return cleaned


def extract_goods_id(url: str) -> str:
    parsed = urlparse(url)
    query = parse_qs(parsed.query)
    for key in ("goods_id", "goodsId", "gid"):
        if key in query and query[key]:
            match = re.search(r"\d{5,}", str(query[key][0]))
            if match:
                return match.group(0)
    for pattern in (r"goods_id=(\d{5,})", r"/goods/(\d{5,})", r"goods/(\d{5,})"):
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    return ""


def canonicalize_pdd_goods_url(url: str) -> str:
    goods_id = extract_goods_id(url)
    if not goods_id:
        return url
    return f"https://mobile.yangkeduo.com/goods.html?goods_id={goods_id}"


def parse_cookie_header(cookie_text: str) -> dict[str, str]:
    cookies: dict[str, str] = {}
    if not cookie_text.strip():
        return cookies
    parts = cookie_text.split(";")
    for part in parts:
        if "=" not in part:
            continue
        k, v = part.split("=", 1)
        key = k.strip()
        val = v.strip()
        if key:
            cookies[key] = val
    return cookies


def fetch_html(url: str, cookie_text: str = "") -> tuple[str, str]:
    headers = {"User-Agent": USER_AGENT}
    if cookie_text.strip():
        headers["Cookie"] = cookie_text.strip()
    resp = requests.get(url, headers=headers, timeout=20, allow_redirects=True)
    resp.raise_for_status()
    return resp.text, resp.url


def meta_values(soup: BeautifulSoup, keys: list[str]) -> list[str]:
    values: list[str] = []
    for key in keys:
        for tag in soup.find_all("meta", attrs={"property": key}):
            content = (tag.get("content") or "").strip()
            if content:
                values.append(content)
        for tag in soup.find_all("meta", attrs={"name": key}):
            content = (tag.get("content") or "").strip()
            if content:
                values.append(content)
    seen = set()
    uniq = []
    for v in values:
        if v not in seen:
            uniq.append(v)
            seen.add(v)
    return uniq


def uniq_by_path(items: list[str]) -> list[str]:
    out: list[str] = []
    seen = set()
    for item in items:
        normalized = item.split("?")[0]
        if normalized not in seen:
            seen.add(normalized)
            out.append(item)
    return out


def classify_media_urls(candidates: list[str]) -> tuple[list[str], list[str]]:
    images: list[str] = []
    videos: list[str] = []
    for raw in candidates:
        url = normalize_candidate_url(raw.strip())
        if not url.startswith("http"):
            continue
        low = url.lower()
        if STATIC_ASSET_EXT_PATTERN.search(low):
            continue

        parsed = urlparse(url)
        path = parsed.path.lower()

        if IMAGE_EXT_PATTERN.search(low):
            images.append(url)
            continue
        if VIDEO_EXT_PATTERN.search(low):
            videos.append(url)
            continue
        # Avoid false positives like "svideo_index.js".
        if any(h in low for h in VIDEO_HINTS) and any(
            token in path for token in ("/video", "video-", "/play", "m3u8", "mp4")
        ):
            videos.append(url)
            continue
        if any(h in low for h in IMAGE_HINTS) and any(
            token in path for token in ("/image", "/img", "cover", "thumb", "pic")
        ):
            images.append(url)
    return uniq_by_path(images), uniq_by_path(videos)


def extract_urls_from_text(text: str) -> list[str]:
    normalized = text.replace("\\u002F", "/").replace("\\/", "/")
    return URL_PATTERN.findall(normalized)


def normalize_candidate_url(value: str) -> str:
    v = value.strip()
    if v.startswith("//"):
        return f"https:{v}"
    return v


def extract_media_from_json_obj(obj: Any, key_path: str = "") -> tuple[list[str], list[str]]:
    images: list[str] = []
    videos: list[str] = []

    def classify_by_key(path: str, value: str) -> None:
        low_path = path.lower()
        url = normalize_candidate_url(value)
        if not url.startswith("http"):
            return
        if any(h in low_path for h in VIDEO_HINTS):
            videos.append(url)
            return
        if any(h in low_path for h in IMAGE_HINTS):
            images.append(url)
            return
        classified_images, classified_videos = classify_media_urls([url])
        images.extend(classified_images)
        videos.extend(classified_videos)

    if isinstance(obj, dict):
        for k, v in obj.items():
            next_path = f"{key_path}.{k}" if key_path else str(k)
            sub_images, sub_videos = extract_media_from_json_obj(v, next_path)
            images.extend(sub_images)
            videos.extend(sub_videos)
    elif isinstance(obj, list):
        for idx, item in enumerate(obj):
            next_path = f"{key_path}[{idx}]"
            sub_images, sub_videos = extract_media_from_json_obj(item, next_path)
            images.extend(sub_images)
            videos.extend(sub_videos)
    elif isinstance(obj, str):
        classify_by_key(key_path, obj)
        for url in extract_urls_from_text(obj):
            classify_by_key(key_path, url)

    return uniq_by_path(images), uniq_by_path(videos)


def filter_valid_video_urls(urls: list[str]) -> list[str]:
    valid: list[str] = []
    for raw in urls:
        url = normalize_candidate_url(raw.strip())
        if not url.startswith("http"):
            continue
        low = url.lower()
        if STATIC_ASSET_EXT_PATTERN.search(low):
            continue
        if VIDEO_EXT_PATTERN.search(low):
            valid.append(url)
            continue
        parsed = urlparse(url)
        path = parsed.path.lower()
        if any(token in path for token in ("/video", "video-", "/play")) and any(
            token in low for token in ("m3u8", "mp4", "video")
        ):
            valid.append(url)
    return uniq_by_path(valid)


def extract_from_html(html: str) -> tuple[str, list[str], list[str]]:
    soup = BeautifulSoup(html, "lxml")

    title = ""
    title_candidates = meta_values(soup, ["og:title", "twitter:title"])
    if title_candidates:
        title = title_candidates[0]
    elif soup.title and soup.title.text:
        title = soup.title.text.strip()

    image_candidates = meta_values(soup, ["og:image", "twitter:image"])
    video_candidates = meta_values(soup, ["og:video", "og:video:url", "twitter:player"])

    for img in soup.find_all("img"):
        src = normalize_candidate_url((img.get("src") or img.get("data-src") or img.get("data-original") or "").strip())
        if src.startswith("http"):
            image_candidates.append(src)
    for video in soup.find_all("video"):
        src = normalize_candidate_url((video.get("src") or "").strip())
        if src.startswith("http"):
            video_candidates.append(src)
        for source in video.find_all("source"):
            source_src = normalize_candidate_url((source.get("src") or "").strip())
            if source_src.startswith("http"):
                video_candidates.append(source_src)

    script_text = " ".join(script.get_text(" ", strip=True) for script in soup.find_all("script"))
    script_urls = extract_urls_from_text(script_text)
    classified_images, classified_videos = classify_media_urls(script_urls)
    image_candidates.extend(classified_images)
    video_candidates.extend(classified_videos)
    return title, uniq_by_path(image_candidates), uniq_by_path(video_candidates)


def parse_static(source_url: str, cookie_text: str = "") -> ProductInfo:
    info = ProductInfo(source_url=source_url)
    html, final_url = fetch_html(source_url, cookie_text=cookie_text)
    info.final_url = final_url
    title, images, videos = extract_from_html(html)
    info.title = title
    all_images = uniq_by_path(images)
    all_videos = uniq_by_path(videos)
    info.images = all_images[:6]
    info.videos = all_videos[:3]
    info.raw = {
        "html_length": len(html),
        "method": "static",
        "video_candidates": all_videos[:12],
        "image_candidates": all_images[:12],
    }
    return info


def parse_dynamic_with_playwright(
    source_url: str,
    cookie_text: str = "",
    live_page: Optional[Any] = None,
) -> ProductInfo:
    info = ProductInfo(source_url=source_url)

    network_urls: list[str] = []
    response_urls: list[str] = []
    json_urls: list[str] = []
    json_images: list[str] = []
    json_videos: list[str] = []
    should_close = False
    if live_page is not None:
        page = live_page
    else:
        try:
            from playwright.sync_api import sync_playwright
        except Exception as exc:
            raise RuntimeError("æœªå®‰è£… playwrightï¼Œè¯·å…ˆæ‰§è¡Œ: playwright install chromium") from exc
        headless = os.getenv("PLAYWRIGHT_HEADLESS", "1").strip().lower() not in {"0", "false", "no"}
        p = sync_playwright().start()
        browser = p.chromium.launch(
            headless=headless,
            args=[
                "--disable-blink-features=AutomationControlled",
                "--no-first-run",
                "--no-default-browser-check",
            ],
        )
        context_kwargs: dict[str, Any] = {
            "user_agent": DESKTOP_USER_AGENT,
            "viewport": {"width": 1280, "height": 900},
            "locale": "zh-CN",
        }
        if os.path.exists(STORAGE_STATE_FILE):
            context_kwargs["storage_state"] = STORAGE_STATE_FILE
        context = browser.new_context(**context_kwargs)
        apply_anti_detection_scripts(context)
        page = context.new_page()
        should_close = True

    context = page.context

    def safe_goto(target_url: str) -> None:
        page.goto(target_url, wait_until="domcontentloaded", timeout=45000)
        if is_blocked_jump_url(page.url):
            page.goto(source_url, wait_until="domcontentloaded", timeout=45000)

    def on_request(request: Any) -> None:
        req_url = request.url
        if req_url.startswith("http"):
            network_urls.append(req_url)

    def on_response(response: Any) -> None:
        res_url = response.url
        if res_url.startswith("http"):
            response_urls.append(res_url)
        content_type = (response.headers.get("content-type") or "").lower()
        resource_type = response.request.resource_type
        if resource_type not in {"xhr", "fetch"} and "json" not in content_type:
            return
        if len(json_urls) >= 80:
            return
        try:
            body = response.text()
        except Exception:
            return
        if "http" not in body:
            return
        body_low = body.lower()
        if not any(k in body_low for k in ("video", "image", "goods", "mp4", "m3u8")):
            return
        json_urls.extend(extract_urls_from_text(body))
        try:
            payload = json.loads(body)
        except Exception:
            return
        extracted_images, extracted_videos = extract_media_from_json_obj(payload)
        json_images.extend(extracted_images)
        json_videos.extend(extracted_videos)

    def on_popup(popup: Any) -> None:
        # Some pages open login windows during click simulation; keep extraction on the main page.
        try:
            popup.close()
        except Exception:
            pass

    def on_context_page(new_page: Any) -> None:
        if new_page == page:
            return
        try:
            new_page.close()
        except Exception:
            pass

    page.on("request", on_request)
    page.on("response", on_response)
    page.on("popup", on_popup)
    context.on("page", on_context_page)
    final_url = source_url
    html = ""
    page_assets: dict[str, list[str]] = {"imgs": [], "videos": [], "links": []}
    try:
        # If we already have a logged-in live page with visible content, avoid extra navigation.
        use_current_page_first = False
        if live_page is not None:
            try:
                use_current_page_first = not page_looks_logged_out(page)
            except Exception:
                use_current_page_first = False

        if not use_current_page_first:
            safe_goto(source_url)

        try:
            page.wait_for_load_state("networkidle", timeout=8000)
        except Exception:
            pass
        page.wait_for_timeout(2500)

        # è§¦å‘é¦–å±åçš„æ‡’åŠ è½½ç´ æ
        page.mouse.wheel(0, 1800)
        page.wait_for_timeout(1200)
        page.mouse.wheel(0, -800)
        page.wait_for_timeout(500)
        page.mouse.wheel(0, 2200)
        page.wait_for_timeout(800)

        # Optional aggressive click probing. Disabled by default to avoid opening login popups.
        click_probe_enabled = os.getenv("PLAYWRIGHT_CLICK_PROBE", "0").strip().lower() in {"1", "true", "yes"}
        if click_probe_enabled:
            for selector in ("video", "[class*=video-play]", "[class*=player]", "button[aria-label*=æ’­æ”¾]"):
                locator = page.locator(selector).first
                try:
                    if locator.is_visible(timeout=500):
                        locator.click(timeout=800, force=True)
                        page.wait_for_timeout(500)
                except Exception:
                    pass

        final_url = page.url
        html = page.content()

        page_assets = page.evaluate(
            """() => {
                const imgs = Array.from(document.querySelectorAll("img"))
                  .map(n => n.currentSrc || n.src || n.getAttribute("data-src") || "")
                  .filter(Boolean);
                const videos = Array.from(document.querySelectorAll("video"))
                  .map(n => n.currentSrc || n.src || "")
                  .filter(Boolean);
                const links = Array.from(document.querySelectorAll("source"))
                  .map(n => n.src || "")
                  .filter(Boolean);
                return { imgs, videos, links };
            }"""
        )
    finally:
        try:
            page.remove_listener("request", on_request)
            page.remove_listener("response", on_response)
            page.remove_listener("popup", on_popup)
        except Exception:
            pass
        try:
            context.remove_listener("page", on_context_page)
        except Exception:
            pass
    if should_close:
        try:
            page.context.close()
        except Exception:
            pass
        try:
            p.stop()
        except Exception:
            pass

    title, images, videos = extract_from_html(html)
    images.extend(page_assets.get("imgs", []))
    videos.extend(page_assets.get("videos", []))
    videos.extend(page_assets.get("links", []))
    images.extend(json_images)
    videos.extend(json_videos)
    all_network = network_urls + response_urls + json_urls
    net_images, net_videos = classify_media_urls(all_network)
    images.extend(net_images)
    videos.extend(net_videos)

    info.final_url = final_url
    info.title = title
    all_images = uniq_by_path(images)
    all_videos = uniq_by_path(videos)
    info.images = all_images[:6]
    info.videos = all_videos[:3]
    info.raw = {
        "html_length": len(html),
        "method": "playwright",
        "network_urls_count": len(uniq_by_path(all_network)),
        "json_video_candidates": len(uniq_by_path(json_videos)),
        "video_candidates": all_videos[:12],
        "image_candidates": all_images[:12],
    }
    return info


def score_info(info: ProductInfo) -> int:
    title_bonus = 0
    if info.title and info.title != "æ‹¼å¤šå¤šå•†åŸ":
        title_bonus = 1
    return len(info.videos) * 100 + len(info.images) * 10 + title_bonus


def merge_info(base: ProductInfo, incoming: ProductInfo, source_label: str) -> None:
    if incoming.title and (not base.title or base.title == "æ‹¼å¤šå¤šå•†åŸ"):
        base.title = incoming.title
    if incoming.final_url:
        base.final_url = incoming.final_url

    merged_images = uniq_by_path(base.images + incoming.images)
    merged_videos = uniq_by_path(base.videos + incoming.videos)
    base.images = merged_images[:6]
    base.videos = merged_videos[:3]

    base_video_candidates = base.raw.get("video_candidates", [])
    incoming_video_candidates = incoming.raw.get("video_candidates", [])
    base.raw["video_candidates"] = uniq_by_path(base_video_candidates + incoming_video_candidates)[:12]
    base_image_candidates = base.raw.get("image_candidates", [])
    incoming_image_candidates = incoming.raw.get("image_candidates", [])
    base.raw["image_candidates"] = uniq_by_path(base_image_candidates + incoming_image_candidates)[:12]

    attempts = base.raw.get("merge_from", [])
    attempts.append(source_label)
    base.raw["merge_from"] = attempts


def ensure_login_browser_session() -> dict[str, Any]:
    try:
        from playwright.sync_api import sync_playwright
    except Exception as exc:
        raise RuntimeError("æœªå®‰è£… playwrightï¼Œè¯·å…ˆæ‰§è¡Œ: playwright install chromium") from exc

    pw = sync_playwright().start()
    browser = None
    context = None
    os.makedirs(PLAYWRIGHT_USER_DATA_DIR, exist_ok=True)
    context_kwargs: dict[str, Any] = {
        "headless": False,
        "locale": "zh-CN",
        "args": [
            "--disable-blink-features=AutomationControlled",
            "--no-first-run",
            "--no-default-browser-check",
        ],
    }
    try:
        # Use a persistent browser profile to keep login/session data across runs.
        context = pw.chromium.launch_persistent_context(
            user_data_dir=PLAYWRIGHT_USER_DATA_DIR,
            channel="chrome",
            **context_kwargs,
        )
    except Exception:
        browser = pw.chromium.launch(
            headless=False,
            args=[
                "--disable-blink-features=AutomationControlled",
                "--no-first-run",
                "--no-default-browser-check",
            ],
        )
        fallback_context_kwargs: dict[str, Any] = {
            "locale": "zh-CN",
        }
        if os.path.exists(STORAGE_STATE_FILE):
            fallback_context_kwargs["storage_state"] = STORAGE_STATE_FILE
        context = browser.new_context(**fallback_context_kwargs)
    apply_anti_detection_scripts(context)

    def on_dialog(dialog: Any) -> None:
        try:
            dialog.dismiss()
        except Exception:
            pass

    page = context.pages[0] if context.pages else context.new_page()
    page.on("dialog", on_dialog)

    return {"pw": pw, "browser": browser, "context": context, "page": page}


def close_login_browser_session(session: dict[str, Any]) -> None:
    context = session.get("context")
    browser = session.get("browser")
    pw = session.get("pw")
    try:
        if context:
            context.storage_state(path=STORAGE_STATE_FILE)
            context.close()
    except Exception:
        pass
    try:
        if browser:
            browser.close()
    except Exception:
        pass
    try:
        if pw:
            pw.stop()
    except Exception:
        pass
    # Do not force-kill browser processes here; graceful close avoids "restore pages" prompts.


def browser_session_alive(session: Optional[dict[str, Any]]) -> bool:
    if not session:
        return False
    context = session.get("context")
    if context is None:
        return False
    try:
        if context.is_closed():
            return False
    except Exception:
        return False
    page = session.get("page")
    if page is None:
        return True
    try:
        if page.is_closed():
            return True
    except Exception:
        return True
    return True


def page_looks_blank(page: Any) -> bool:
    try:
        return bool(
            page.evaluate(
                """() => {
                    const body = document.body;
                    if (!body) return true;
                    const hasMedia = !!document.querySelector("img, video, source, iframe, canvas");
                    const textLen = (body.innerText || "").trim().length;
                    const childCount = body.children ? body.children.length : 0;
                    return !hasMedia && textLen === 0 && childCount <= 1;
                }"""
            )
        )
    except Exception:
        return False


def ensure_session_page(browser_session: dict[str, Any]) -> Any:
    context = browser_session.get("context")
    page = browser_session.get("page")
    if context is None:
        raise RuntimeError("æµè§ˆå™¨ä¸Šä¸‹æ–‡ä¸å¯ç”¨ï¼Œè¯·é‡æ–°æ‰“å¼€ç™»å½•æµè§ˆå™¨ã€‚")

    if page is not None:
        try:
            if not page.is_closed():
                return page
        except Exception:
            pass

    for candidate in context.pages:
        try:
            if not candidate.is_closed():
                browser_session["page"] = candidate
                return candidate
        except Exception:
            continue

    new_page = context.new_page()
    browser_session["page"] = new_page
    return new_page


def close_extra_pages(context: Any, keep_page: Any) -> None:
    try:
        pages = list(context.pages)
    except Exception:
        return
    for p in pages:
        if p == keep_page:
            continue
        try:
            p.close()
        except Exception:
            pass


def goto_with_recover(url: str, browser_session: dict[str, Any]) -> tuple[dict[str, Any], Any]:
    page = ensure_session_page(browser_session)
    try:
        page.goto(url, wait_until="domcontentloaded", timeout=45000)
        page.wait_for_timeout(1200)
        if page_looks_blank(page):
            page.reload(wait_until="domcontentloaded", timeout=45000)
            page.wait_for_timeout(1200)
        return browser_session, page
    except Exception as exc:
        msg = str(exc)
        if "Target page, context or browser has been closed" not in msg:
            raise
        # Recover within the same context first; only recreate session if context is gone.
        try:
            page = ensure_session_page(browser_session)
            page.goto(url, wait_until="domcontentloaded", timeout=45000)
            return browser_session, page
        except Exception:
            raise RuntimeError("å½“å‰ç™»å½•ä¼šè¯ä¸å¯ç”¨ï¼Œè¯·ç‚¹å‡»â€œå…³é—­ç™»å½•æµè§ˆå™¨â€åå†é‡æ–°ç‚¹å‡»â€œå¼€å§‹ç”Ÿæˆâ€ã€‚")


def page_looks_logged_out(page: Any) -> bool:
    try:
        current_url = (page.url or "").lower()
    except Exception:
        return True

    parsed = urlparse(current_url)
    path = parsed.path or ""

    # If the page already renders media/content, treat it as ready even if query contains "login".
    try:
        has_content = page.evaluate(
            """() => {
                const hasVideo = !!document.querySelector('video[src], video source[src], source[src*=".mp4"], source[src*=".m3u8"]');
                const bodyText = (document.body && document.body.innerText) ? document.body.innerText : "";
                const hasGoodsSignals = /ç«‹å³æ‹¼å•|å·²æ‹¼|åˆ¸å|çœ‹è§†é¢‘äº«ä¸“å±ä¼˜æƒ |å•†å“/.test(bodyText);
                return hasVideo || hasGoodsSignals;
            }"""
        )
        if bool(has_content):
            return False
    except Exception:
        pass

    # Only detect login pages by path/host level features, not by query params like refer_page_name=login.
    if any(k in path for k in ("/login", "/passport", "/oauth", "/verify")):
        return True
    if "needs_login=1" in current_url and "goods_id=" not in current_url and "fyxmkief" not in path:
        return True
    if is_blocked_jump_url(current_url):
        return True

    try:
        has_login_ui = page.evaluate(
            """() => {
                const text = document.body ? document.body.innerText : "";
                if (!text) return false;
                const loginWords = /ç™»å½•|æ³¨å†Œ|æ‰‹æœºå·ç™»å½•|éªŒè¯ç ç™»å½•|è¯·å…ˆç™»å½•/;
                const hasLoginForm = !!document.querySelector('input[type="password"], input[type="tel"], input[name*="phone"], input[name*="mobile"]');
                return loginWords.test(text) && hasLoginForm;
            }"""
        )
        return bool(has_login_ui)
    except Exception:
        return True


def has_login_cookies(page: Any) -> bool:
    try:
        cookies = page.context.cookies()
    except Exception:
        return False
    cookie_names = {c.get("name", "").lower() for c in cookies}
    # Common auth/session cookie signals for PDD web sessions.
    signals = {"api_uid", "pdd_user_id", "pdd_user_uin", "_nano_fp", "ua"}
    return len(cookie_names.intersection(signals)) >= 1


def parse_product_info(
    source_url: str,
    cookie_text: str = "",
    live_page: Optional[Any] = None,
) -> ProductInfo:
    canonical_url = canonicalize_pdd_goods_url(source_url)
    candidate_urls: list[str] = [source_url]
    if canonical_url != source_url:
        candidate_urls.append(canonical_url)

    static_infos: list[ProductInfo] = []
    static_errors: list[str] = []
    for u in candidate_urls:
        try:
            static_info = parse_static(u, cookie_text=cookie_text)
            static_infos.append(static_info)
        except Exception as exc:
            static_errors.append(f"{u} -> {exc}")

    if not static_infos:
        raise RuntimeError("é™æ€æŠ“å–å…¨éƒ¨å¤±è´¥: " + " | ".join(static_errors))

    best = max(static_infos, key=score_info)
    info = ProductInfo(
        source_url=source_url,
        final_url=best.final_url,
        title=best.title,
        images=list(best.images),
        videos=list(best.videos),
        raw=dict(best.raw),
    )
    info.raw["method"] = "static"
    info.raw["canonical_url"] = canonical_url
    info.raw["attempted_urls"] = candidate_urls
    info.raw["needs_login"] = "needs_login=1" in source_url
    if static_errors:
        info.raw["static_errors"] = static_errors

    need_dynamic = (not info.title) or (len(info.images) < 1) or (len(info.videos) < 1)
    if not need_dynamic:
        return info

    dynamic_attempt_urls: list[str] = []
    first_try_url = info.final_url or source_url
    if not is_blocked_jump_url(first_try_url):
        dynamic_attempt_urls.append(first_try_url)
    else:
        dynamic_attempt_urls.append(source_url)
    for u in candidate_urls:
        if u not in dynamic_attempt_urls and not is_blocked_jump_url(u):
            dynamic_attempt_urls.append(u)

    dynamic_logs: list[str] = []
    for idx, u in enumerate(dynamic_attempt_urls):
        try:
            dynamic_info = parse_dynamic_with_playwright(
                u,
                cookie_text=cookie_text,
                live_page=live_page,
            )
            merge_info(info, dynamic_info, source_label=f"dynamic_{idx}:{u}")
            info.raw["network_urls_count"] = max(
                int(info.raw.get("network_urls_count", 0)),
                int(dynamic_info.raw.get("network_urls_count", 0)),
            )
            info.raw["json_video_candidates"] = max(
                int(info.raw.get("json_video_candidates", 0)),
                int(dynamic_info.raw.get("json_video_candidates", 0)),
            )
            dynamic_logs.append(f"{u} -> ok")
            info.raw["fallback"] = "playwright"
            info.raw["method"] = "hybrid(static+playwright)"
            if info.videos:
                break
        except Exception as exc:
            dynamic_logs.append(f"{u} -> failed: {exc}")

    if dynamic_logs:
        info.raw["dynamic_attempts"] = dynamic_logs
    if "fallback" not in info.raw and dynamic_logs:
        info.raw["fallback"] = "playwright_failed: " + " | ".join(dynamic_logs)

    info.videos = filter_valid_video_urls(info.videos)[:3]
    info.raw["video_candidates"] = filter_valid_video_urls(info.raw.get("video_candidates", []))[:12]

    return info


def fallback_copy(info: ProductInfo) -> dict[str, str]:
    title = info.title or "è¯¥å•†å“"
    points = (
        f"1) ç”¨æˆ·å…³æ³¨ç‚¹ï¼š{title}æ˜¯å¦çœŸæœ‰æ€§ä»·æ¯”ã€‚\n"
        "2) æ ¸å¿ƒå–ç‚¹ï¼šä»·æ ¼é—¨æ§›ä½ã€ä¸‹å•é“¾è·¯çŸ­ã€é€‚åˆå¿«é€Ÿå†³ç­–ã€‚\n"
        "3) ä¸‹å•è§¦å‘ï¼šé™æ—¶ã€é™é‡ã€çœŸå®ä½¿ç”¨åœºæ™¯ã€‚"
    )
    script = (
        f"å¼€åœº3ç§’ï¼šä»Šå¤©æµ‹ä¸€ä¸ªçˆ†æ¬¾ï¼Œåå­—å«ã€Š{title}ã€‹ã€‚\n"
        "ä¸­æ®µ15ç§’ï¼šæˆ‘å…ˆè¯´ç»“è®ºï¼Œå®ƒæœ€å¤§çš„ä¼˜åŠ¿æ˜¯å…¥æ‰‹é—¨æ§›ä½ï¼ŒåŠŸèƒ½è¦†ç›–å¸¸è§éœ€æ±‚ã€‚"
        "å¦‚æœä½ è·Ÿæˆ‘ä¸€æ ·è¿½æ±‚çœé’±çœäº‹ï¼Œè¿™ä¸ªé…ç½®å·²ç»å¤Ÿç”¨ã€‚\n"
        "æ”¶å°¾12ç§’ï¼šé€‚åˆå­¦ç”Ÿå…šã€ç§Ÿæˆ¿å…šã€å’Œç¬¬ä¸€æ¬¡å°è¯•çš„äººç¾¤ã€‚"
        "æƒ³è¦é“¾æ¥æˆ‘æ”¾åœ¨è¯„è®ºåŒºï¼Œå…ˆé¢†åˆ¸å†ä¸‹å•ã€‚"
    )
    xhs = (
        f"æ ‡é¢˜å»ºè®®ï¼šæŒ–åˆ°å®äº†ï½œ{title}å€¼ä¸å€¼ï¼Ÿ\n"
        "æ­£æ–‡å»ºè®®ï¼š\n"
        "æœ€è¿‘åœ¨åšå¹³ä»·å¥½ç‰©æµ‹è¯„ï¼Œè¿™ä¸ªæˆ‘å®é™…çœ‹ä¸‹æ¥æœ‰3ä¸ªä¼˜ç‚¹ï¼š\n"
        "1. é¢„ç®—å‹å¥½\n2. ä½¿ç”¨é—¨æ§›ä½\n3. æ—¥å¸¸åœºæ™¯è¦†ç›–å¹¿\n"
        "ä¸å¤¸å¼ ä¸è¸©é›·ï¼Œå»ºè®®å…ˆé¢†åˆ¸å†å†³å®šã€‚"
    )
    return {"selling_points": points, "script_30s": script, "xhs_rewrite": xhs}


def generate_ai_copy(info: ProductInfo) -> dict[str, str]:
    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    if not api_key:
        return fallback_copy(info)

    try:
        from openai import OpenAI
    except Exception:
        return fallback_copy(info)

    model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
    client = OpenAI(api_key=api_key)
    prompt = {
        "title": info.title,
        "images": info.images,
        "videos": info.videos,
        "goal": [
            "å–ç‚¹æ‹†è§£ï¼ˆ3-5æ¡ï¼‰",
            "30ç§’å¸¦è´§è„šæœ¬ï¼ˆåˆ†æ®µï¼‰",
            "å°çº¢ä¹¦ç‰ˆæœ¬æ”¹å†™ï¼ˆæ ‡é¢˜+æ­£æ–‡ï¼‰",
        ],
    }
    resp = client.chat.completions.create(
        model=model,
        temperature=0.7,
        response_format={"type": "json_object"},
        messages=[
            {
                "role": "system",
                "content": (
                    "ä½ æ˜¯ç”µå•†å†…å®¹ç­–ç•¥å¸ˆã€‚è¯·æ ¹æ®å•†å“ä¿¡æ¯è¾“å‡ºJSONï¼Œå­—æ®µå›ºå®šä¸º"
                    "selling_points, script_30s, xhs_rewriteã€‚å†…å®¹ä½¿ç”¨ç®€ä½“ä¸­æ–‡ã€‚"
                ),
            },
            {"role": "user", "content": json.dumps(prompt, ensure_ascii=False)},
        ],
    )
    content = resp.choices[0].message.content or "{}"
    data = json.loads(content)
    if not all(key in data for key in ["selling_points", "script_30s", "xhs_rewrite"]):
        return fallback_copy(info)
    return {
        "selling_points": str(data["selling_points"]),
        "script_30s": str(data["script_30s"]),
        "xhs_rewrite": str(data["xhs_rewrite"]),
    }


def main() -> None:
    st.set_page_config(page_title="PDD å†…å®¹ç”Ÿæˆ MVP", page_icon="ğŸ›ï¸", layout="wide")
    st.title("æ‹¼å¤šå¤šå•†å“å†…å®¹ç”Ÿæˆ MVP")
    st.caption("è¾“å…¥æ‹¼å¤šå¤šå•†å“é“¾æ¥ï¼Œè‡ªåŠ¨æå–æ ‡é¢˜/ä¸»å›¾/è§†é¢‘ï¼Œå¹¶ç”Ÿæˆå†…å®¹æ–‡æ¡ˆã€‚")

    view_mode = st.sidebar.radio("ç•Œé¢æ¨¡å¼", ["ç”¨æˆ·è§†å›¾", "ç®¡ç†å‘˜è§†å›¾"], index=0)
    is_admin = view_mode == "ç®¡ç†å‘˜è§†å›¾"
    admin_key = os.getenv("ADMIN_VIEW_KEY", "").strip()
    if is_admin and admin_key:
        key_input = st.sidebar.text_input("ç®¡ç†å‘˜å£ä»¤", type="password")
        if key_input != admin_key:
            st.warning("ç®¡ç†å‘˜å£ä»¤é”™è¯¯ï¼Œå·²åˆ‡æ¢ä¸ºç”¨æˆ·è§†å›¾ã€‚")
            is_admin = False

    raw_input = st.text_area(
        "å•†å“é“¾æ¥ï¼ˆå¯ç²˜è´´å¾®ä¿¡åˆ†äº«æ–‡æœ¬ï¼‰",
        placeholder="ä¾‹å¦‚ï¼šhttps://mobile.yangkeduo.com/goods.html?goods_id=xxxx",
        height=90,
    )

    cookie_input = ""
    if is_admin:
        cookie_input = st.text_area(
            "å¯é€‰ï¼šæ‹¼å¤šå¤š Cookieï¼ˆç”¨äºéœ€è¦ç™»å½•æ€çš„é“¾æ¥ï¼‰",
            placeholder="ä¾‹å¦‚ï¼šapi_uid=xxx; PDDAccessToken=xxx; ...",
            height=80,
        )
    if "browser_session" not in st.session_state:
        st.session_state["browser_session"] = None

    action_col, login_col, close_col, cleanup_col, force_cleanup_col = st.columns([1, 1, 1, 1, 1])
    with action_col:
        run = st.button("å¼€å§‹ç”Ÿæˆ", type="primary")
    with login_col:
        login_confirmed = st.toggle(
            "ç™»å½•çŠ¶æ€ï¼šæˆ‘å·²å®Œæˆç™»å½•",
            value=False,
            key="login_confirmed_toggle",
            help="ç‚¹å¼€=å·²ç¡®è®¤ç™»å½•ï¼›å…³é—­=æœªç¡®è®¤ç™»å½•ã€‚",
        )
        login_state_text = "å·²ç‚¹å¼€ï¼ˆå·²ç¡®è®¤ç™»å½•ï¼‰" if login_confirmed else "æœªç‚¹å¼€ï¼ˆæœªç¡®è®¤ç™»å½•ï¼‰"
        login_state_style = (
            "background:#e8f7ee;color:#0f6b38;border:1px solid #a7dfbe;"
            if login_confirmed
            else "background:#fff3e8;color:#9c4b00;border:1px solid #ffc999;"
        )
        st.markdown(
            (
                "<div style='margin-top:4px;padding:6px 10px;border-radius:8px;"
                f"font-weight:600;display:inline-block;{login_state_style}'>"
                f"å½“å‰çŠ¶æ€ï¼š{login_state_text}</div>"
            ),
            unsafe_allow_html=True,
        )
        st.caption("å‹¾é€‰åè¯·å†ç‚¹å‡»â€œå¼€å§‹ç”Ÿæˆâ€ä»¥ç»§ç»­é‡‡é›†ã€‚")
    with close_col:
        close_browser = st.button("å…³é—­ç™»å½•æµè§ˆå™¨")
    with cleanup_col:
        cleanup_browser = st.button("æ¸…ç†æ®‹ç•™æµ‹è¯•æµè§ˆå™¨")
    with force_cleanup_col:
        force_cleanup = st.button("å¼ºåŠ›æ¸…ç†Chromium")

    if close_browser:
        session = st.session_state.get("browser_session")
        if browser_session_alive(session):
            close_login_browser_session(session)
            st.success("å·²å…³é—­ç™»å½•æµè§ˆå™¨ä¼šè¯ã€‚")
        st.session_state["browser_session"] = None

    if cleanup_browser:
        session = st.session_state.get("browser_session")
        if browser_session_alive(session):
            close_login_browser_session(session)
            st.session_state["browser_session"] = None
        cleanup_result = cleanup_stale_test_browsers()
        matched_patterns = cleanup_result.get("matched_patterns", [])
        errors = cleanup_result.get("errors", [])
        if matched_patterns:
            st.success(f"å·²æ¸…ç†æ®‹ç•™è¿›ç¨‹ï¼ˆå‘½ä¸­{len(matched_patterns)}ä¸ªç‰¹å¾ï¼‰ã€‚")
        else:
            st.warning("æœªå‘½ä¸­å¯æ¸…ç†çš„æµ‹è¯•è¿›ç¨‹ç‰¹å¾ã€‚å¯å°è¯•â€œå¼ºåŠ›æ¸…ç†Chromiumâ€ã€‚")
        if errors:
            st.error("æ¸…ç†å‘½ä»¤æœ‰å¼‚å¸¸: " + " | ".join(errors))

    if force_cleanup:
        session = st.session_state.get("browser_session")
        if browser_session_alive(session):
            close_login_browser_session(session)
            st.session_state["browser_session"] = None
        kill_result = force_kill_chromium_processes()
        if kill_result.get("killed"):
            st.success("å·²å¼ºåŠ›æ¸…ç†æ‰€æœ‰ Chromium è¿›ç¨‹ã€‚")
        else:
            rc = kill_result.get("returncode")
            err = kill_result.get("stderr")
            if rc == 1:
                st.info("å½“å‰æ²¡æœ‰å¯æ¸…ç†çš„ Chromium è¿›ç¨‹ã€‚")
            else:
                st.error(f"å¼ºåŠ›æ¸…ç†å¤±è´¥ï¼ˆrc={rc}ï¼‰ã€‚{err}")

    if run:
        source_url = normalize_url(raw_input)
        if not source_url:
            st.error("è¯·å…ˆè¾“å…¥æœ‰æ•ˆé“¾æ¥ã€‚")
            return
        url = canonicalize_pdd_goods_url(source_url)
        if url != source_url:
            st.info("å·²è‡ªåŠ¨è½¬æ¢ä¸ºå•†å“ç›´è¾¾é“¾æ¥ï¼Œå‡å°‘ç™»å½•è·³è½¬ã€‚")
        st.info("å·²è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨å¹¶åŠ è½½é“¾æ¥ã€‚è‹¥é¡µé¢æç¤ºç™»å½•ï¼Œè¯·å…ˆå®Œæˆç™»å½•å¹¶å‹¾é€‰â€œæˆ‘å·²å®Œæˆç™»å½•â€ã€‚")
        with st.spinner("æ­£åœ¨æŠ“å–å¹¶ç”Ÿæˆ..."):
            browser_session: Optional[dict[str, Any]] = st.session_state.get("browser_session")
            try:
                if not browser_session_alive(browser_session):
                    if browser_session:
                        close_login_browser_session(browser_session)
                        st.session_state["browser_session"] = None
                    browser_session = ensure_login_browser_session()
                    st.session_state["browser_session"] = browser_session
                    st.info("å·²æ–°å»ºç™»å½•æµè§ˆå™¨ä¼šè¯ã€‚")
                else:
                    st.info("å¤ç”¨å·²æ‰“å¼€çš„ç™»å½•æµè§ˆå™¨ä¼šè¯ã€‚")
                browser_session, page = goto_with_recover(url, browser_session)
                st.session_state["browser_session"] = browser_session

                # Hard gate: never collect unless user explicitly confirms login.
                if not login_confirmed:
                    try:
                        page.context.storage_state(path=STORAGE_STATE_FILE)
                    except Exception:
                        pass
                    st.warning("æœªå‹¾é€‰â€œæˆ‘å·²å®Œæˆç™»å½•â€ï¼Œæœ¬æ¬¡ä¸ä¼šæ‰§è¡Œé‡‡é›†ã€‚è¯·å®Œæˆç™»å½•å¹¶å‹¾é€‰åå†ç‚¹å‡»å¼€å§‹ç”Ÿæˆã€‚")
                    return

                if login_confirmed:
                    st.info("å·²æŒ‰â€œæˆ‘å·²å®Œæˆç™»å½•â€ç»§ç»­æŠ“å–ã€‚")

                try:
                    page.context.storage_state(path=STORAGE_STATE_FILE)
                except Exception:
                    pass
                active_url = page.url or url
                close_extra_pages(page.context, page)

                info = parse_product_info(
                    active_url,
                    cookie_text=cookie_input,
                    live_page=page,
                )
            except Exception as exc:
                if browser_session and browser_session.get("page"):
                    try:
                        browser_session["page"].context.storage_state(path=STORAGE_STATE_FILE)
                    except Exception:
                        pass
                st.exception(exc)
                return
            copy_result = generate_ai_copy(info)

        st.subheader("æŠ“å–ç»“æœ")
        st.write(f"- æ ‡é¢˜: {info.title or 'æœªæå–åˆ°'}")

        if is_admin:
            with st.expander("ç®¡ç†å‘˜è°ƒè¯•ä¿¡æ¯", expanded=True):
                st.write(f"- è¾“å…¥é“¾æ¥: `{info.source_url}`")
                st.write(f"- æœ€ç»ˆé“¾æ¥: `{info.final_url}`")
                st.write(f"- è§„èŒƒåŒ–é“¾æ¥: `{info.raw.get('canonical_url', info.source_url)}`")
                st.write(f"- æŠ“å–æ–¹å¼: `{info.raw.get('method', 'unknown')}`")
                if "network_urls_count" in info.raw:
                    st.write(f"- åŠ¨æ€ç½‘ç»œURLæ•°: `{info.raw['network_urls_count']}`")
                if "json_video_candidates" in info.raw:
                    st.write(f"- JSONè§†é¢‘å€™é€‰æ•°: `{info.raw['json_video_candidates']}`")
                if "fallback" in info.raw:
                    st.write(f"- åŠ¨æ€æŠ“å–: `{info.raw['fallback']}`")
                if "dynamic_attempts" in info.raw:
                    st.write("- åŠ¨æ€å°è¯•:")
                    for row in info.raw["dynamic_attempts"]:
                        st.write(f"  - {row}")
                st.write("- ç™»å½•ä¼šè¯: `å•æ¬¡ä¼šè¯ï¼ˆç™»å½•æ€å·²æŒä¹…åŒ–ï¼‰`")
                st.write(f"- ç™»å½•çŠ¶æ€å‹¾é€‰: `{'æ˜¯' if login_confirmed else 'å¦'}`")
                if info.raw.get("needs_login"):
                    st.info("è¯¥åˆ†äº«é“¾æ¥åŒ…å« needs_login=1ï¼Œå•†å“è§†é¢‘å¯èƒ½éœ€è¦ç™»å½•æ€æ‰èƒ½è¿”å›ã€‚")
                st.write(f"- CookieçŠ¶æ€: `{'å·²æä¾›' if cookie_input.strip() else 'æœªæä¾›'}`")
                if info.title == "æ‹¼å¤šå¤šå•†åŸ" and not info.videos:
                    st.warning("å½“å‰é“¾æ¥å¯èƒ½è¢«é‡å®šå‘åˆ°å•†åŸé¦–é¡µè€Œéå•†å“è¯¦æƒ…é¡µã€‚å»ºè®®ç²˜è´´åŒ…å« goods_id çš„åˆ†äº«é“¾æ¥ã€‚")

        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**ä¸»å›¾**")
            if info.images:
                for image in info.images:
                    st.image(image, use_container_width=True)
            else:
                st.info("æœªæå–åˆ°ä¸»å›¾ã€‚")
        with col2:
            st.markdown("**è§†é¢‘**")
            if info.videos:
                for video in info.videos:
                    st.video(video)
                if is_admin:
                    st.caption("å·²æå–è§†é¢‘URL")
                    st.code("\n".join(info.videos), language="text")
            else:
                st.info("æœªæå–åˆ°è§†é¢‘ã€‚")
                candidates = info.raw.get("video_candidates", [])
                if is_admin and candidates:
                    st.caption("æ£€æµ‹åˆ°è§†é¢‘å€™é€‰URLï¼ˆå¯æ‰‹åŠ¨éªŒè¯ï¼‰")
                    st.code("\n".join(candidates), language="text")

        st.subheader("AI è¾“å‡º")
        st.markdown("**å–ç‚¹æ‹†è§£**")
        st.write(copy_result["selling_points"])
        st.markdown("**30ç§’å¸¦è´§è„šæœ¬**")
        st.write(copy_result["script_30s"])
        st.markdown("**å°çº¢ä¹¦ç‰ˆæœ¬æ”¹å†™**")
        st.write(copy_result["xhs_rewrite"])

        if not os.getenv("OPENAI_API_KEY"):
            st.warning("æ£€æµ‹åˆ°æœªé…ç½® OPENAI_API_KEYï¼Œå½“å‰å±•ç¤ºçš„æ˜¯æœ¬åœ°æ¨¡æ¿ç”Ÿæˆç»“æœã€‚")


if __name__ == "__main__":
    main()
